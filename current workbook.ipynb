{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TAGS, progressbar, pickle, re, time, json\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from random import randint\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import unicodedata as ud\n",
    "import pandas as pd\n",
    "\n",
    "latin_letters = {}\n",
    "\n",
    "def is_latin(uchr):\n",
    "    try:\n",
    "        return latin_letters[uchr]\n",
    "    except KeyError:\n",
    "        return latin_letters.setdefault(uchr, 'LATIN' in ud.name(uchr))\n",
    "\n",
    "\n",
    "def only_roman_chars(unistr):\n",
    "    return all(is_latin(uchr)\n",
    "               for uchr in unistr\n",
    "               if uchr.isalpha())  # isalpha suggested by John Machin\n",
    "\n",
    "\n",
    "def log(message, level=0):\n",
    "    if level > 0:\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = TAGS.document(path=\"../../datasets/tags-tsv/boylesque/TAGS - boylesque - Archive.tsv\", suppress_warnings=True)\n",
    "ids = tags.ids\n",
    "tweets = TAGS.TweetSet(ids, suppress_warnings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (32697 of 32697) |##################| Elapsed Time: 0:00:35 Time:  0:00:35\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2167\n"
     ]
    }
   ],
   "source": [
    "print(len(all_locations))\n",
    "with open(\"locations.txt\", \"w+\") as f:\n",
    "    f.write(\"\\n\".join(all_locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas.DataFrame.from_dict(all_data).to_csv('boylesque-dataset-Oct-8-10.00.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"locations.txt\", \"r\") as f:\n",
    "    all_locations = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24% (8162 of 32697) |####               | Elapsed Time: 0:00:10 ETA:   0:00:35"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-652-c470ebaa47a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;34m'lng'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlng\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;34m'inferred'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minferred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;34m'date'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y-%m-%d %H:%M:%S'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreated_at\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'%a %b %d %H:%M:%S +0000 %Y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;34m'lang'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         }\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "location_cache_directory = Path('./locations/')\n",
    "\n",
    "def get_location_data(location, geolocator=\"Nominatim\"):\n",
    "    if geolocator == \"Nominatim\":\n",
    "        log(\"Geolocator chosen: Nominatim.\")\n",
    "        sleep_val = randint(2,5)\n",
    "        log(f\"Sleeping for {sleep_val} seconds...\")\n",
    "        time.sleep(sleep_val)\n",
    "        _geolocator = Nominatim(user_agent=\"tags-research-app\")\n",
    "    else:\n",
    "        raise NotImplementedError(\"You can currently only use Nomatim as geolocator in this script.\")\n",
    "    \n",
    "    try:\n",
    "        geocoded_location = _geolocator.geocode(location)\n",
    "    \n",
    "        if geocoded_location is not None:\n",
    "            data = {\n",
    "                'lat': geocoded_location.raw.get('lat', None),\n",
    "                'lng': geocoded_location.raw.get('lon', None),\n",
    "                'geolocator': geolocator,\n",
    "                'type': geocoded_location.raw.get('type', None),\n",
    "                'class': geocoded_location.raw.get('class', None),\n",
    "                'display_name': geocoded_location.raw.get('display_name', None),\n",
    "                'boundingbox': geocoded_location.raw.get('boundingbox', None)\n",
    "            }\n",
    "        else:\n",
    "            data = {\n",
    "                'lat': None, 'lng': None, 'geolocator': None, 'type': None, 'class': None, 'display_name': None, 'boundingbox': None\n",
    "            }\n",
    "    except:\n",
    "        data = {'error': 'timeout'}\n",
    "    \n",
    "    return(data)\n",
    "    \n",
    "def read_cache(cache_file):\n",
    "    with open(cache_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return(data)\n",
    "\n",
    "def save_cache(data, cache_file):\n",
    "    if data is None or not isinstance(data, dict): raise RuntimeError(\"Data must not be empty and must be in dictionary format.\")\n",
    "    try:\n",
    "        with open(cache_file, 'w+') as f:\n",
    "            json.dump(data, f)\n",
    "        return(True)\n",
    "    except:\n",
    "        return(False)\n",
    "    \n",
    "def clean_location(location):\n",
    "    if re.findall(\"\\w,\\w\", location):\n",
    "        location = re.sub(r\"(\\w+),(\\w+)\", r\"\\1, \\2\", location)\n",
    "        location = re.sub(r\"(\\w+),(\\w+)\", r\"\\1, \\2\", location)\n",
    "        location = re.sub(r\"(\\w+),(\\w+)\", r\"\\1, \\2\", location)\n",
    "        location = re.sub(r\"(World)|(Main part→)\", r\"\", location, re.I)\n",
    "    location = TAGS.clean_text(location, no_digits=False) # cleaning up the location\n",
    "    location = location.lower()\n",
    "    return(location)\n",
    "\n",
    "# Set up location_cache_directory\n",
    "if not location_cache_directory.exists(): location_cache_directory.mkdir()\n",
    "\n",
    "# Set up standard variables\n",
    "errors = 0\n",
    "max_errors = 20\n",
    "\n",
    "all_locations = []\n",
    "all_data = []\n",
    "bar = progressbar.ProgressBar(max_value=len(tweets.tweets)).start()\n",
    "for i, tweet in enumerate(tweets.tweets):\n",
    "    inferred = False\n",
    "    bar.update(i)\n",
    "    full_text = tweet._json['full_text'].replace(\"amp\", \"\")\n",
    "    keywords = TAGS.clean_text(full_text)\n",
    "    if 'boylesque' in keywords and not keywords.startswith(\"rt\"): # only include tweets mentioning boylesque + not retweets\n",
    "        geocoder_timeout = False\n",
    "        \n",
    "        # Geolocation\n",
    "        if tweet.geo:\n",
    "            lat = tweet.geo['coordinates'][0]\n",
    "            lng = tweet.geo['coordinates'][1]\n",
    "            inferred = False\n",
    "            geocoder = None\n",
    "        elif tweet.user.location is not None and only_roman_chars(tweet.user.location):\n",
    "            location = clean_location(tweet.user.location)\n",
    "            if len(location) == 0: next\n",
    "            cache_file = location_cache_directory / Path(location + \".json\")\n",
    "            if cache_file.is_file():\n",
    "                geo_data = read_cache(cache_file)\n",
    "                if geo_data.get('lat') == None and geo_data.get('lng') == None and geo_data.get('boundingbox') == None: next\n",
    "                lat = geo_data.get('lat', None)\n",
    "                lng = geo_data.get('lng', None)\n",
    "                inferred = True\n",
    "            else:\n",
    "                geo_data = get_location_data(location)\n",
    "                inferred = True\n",
    "                if not 'error' in geo_data:\n",
    "                    if not save_cache(geo_data, cache_file): raise RuntimeError(\"File could not be saved.\")\n",
    "                    log(f\"Saved: {geo_data} in {cache_file}\", 1)\n",
    "                else:\n",
    "                    errors += 1\n",
    "                    log(f\"Error: {location} has encountered an error ({geo_data['error']}). (error {errors}/{max_errors})\", 1)\n",
    "                    if errors > max_errors: break\n",
    "        elif tweet.user.location is not None and not only_roman_chars(tweet.user.location):\n",
    "            pass # or:\n",
    "                    # print(f\"Location contained non-roman characters: {tweet.user.location}\")\n",
    "        \n",
    "        data = {\n",
    "            'keywords': keywords,\n",
    "            'bounding_box': geo_data.get('boundingbox', None),\n",
    "            'lat': lat,\n",
    "            'lng': lng,\n",
    "            'inferred': inferred,\n",
    "            'date': time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(tweet.created_at,'%a %b %d %H:%M:%S +0000 %Y')),\n",
    "            'lang': tweet.lang\n",
    "        }\n",
    "        all_data.append(data)\n",
    "bar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(all_data)\n",
    "df.to_csv('boylesque-geodata-october-2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from shapely.geometry import Point\n",
    "from shapely.geometry import box\n",
    "\n",
    "point = Point(0.5, 0.5)\n",
    "b = box(-75.7854388,-75.6325211,4.3951997,4.5895768)\n",
    "print(b.contains(point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(-75.7854388, -75.6325211),\n",
    "(4.3951997, 4.5895768),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "locations/colchester.json Colchester, Essex, East of England, England, United Kingdom\n",
      "(min_lng,max_lng,min_lat,max_lat)\n",
      "(0.6993788,1.0268034,51.7657055,51.9771532)\n",
      "51.8896903 0.8994651\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bounding box is: \"south Latitude, north Latitude, west Longitude, east Longitude\"\n",
    "# according to https://wiki.openstreetmap.org/wiki/Bounding_Box\n",
    "for file in Path('./locations/').glob('colchester.json'):\n",
    "    with file.open() as f:\n",
    "        data = json.load(f)\n",
    "        if data['boundingbox'] is not None:\n",
    "            min_lat = data['boundingbox'][0]\n",
    "            max_lat = data['boundingbox'][1]\n",
    "            min_lng = data['boundingbox'][2]\n",
    "            max_lng = data['boundingbox'][3]\n",
    "            print(file, data['display_name'])\n",
    "            print(f\"(min_lng,max_lng,min_lat,max_lat)\")\n",
    "            print(f\"({min_lng},{max_lng},{min_lat},{max_lat})\")\n",
    "            print(data['lat'], data['lng'])\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundingbox(location = None):\n",
    "    if not location: raise RuntimeError(f\"Location must be set to not None.\")\n",
    "    location = clean_location(tweet.user.location)\n",
    "    if len(location):\n",
    "        cache_file = location_cache_directory / Path(location + \".json\")\n",
    "        if cache_file.is_file():\n",
    "            geo_data = read_cache(cache_file)\n",
    "            return_data = geo_data.get('boundingbox', None)\n",
    "            if isinstance(return_data, list): \n",
    "                print(\"return_data is a list!\")\n",
    "                return_data = [float(x) for x in return_data] # returned as list of floats: [NS/latitude min, NS/latitude max; EW/longitude min; EW/longitude max]\n",
    "            return(return_data)\n",
    "        else:\n",
    "            log(f\"Cannot read cache file / file does not exist: {cache_file}\")\n",
    "            return(None)\n",
    "\n",
    "def get_user_location(tweet):\n",
    "    if tweet.user.location is not None:\n",
    "        location = clean_location(tweet.user.location)\n",
    "    else:\n",
    "        return(False)\n",
    "    \n",
    "    if not location or len(location) == 0 or not only_roman_chars(location): # for now, this is the implementation\n",
    "        return(False)\n",
    "    else:\n",
    "        cache_file = location_cache_directory / Path(location + \".json\")\n",
    "        if cache_file.is_file():\n",
    "            geo_data = read_cache(cache_file)\n",
    "            lat = geo_data.get('lat', None)\n",
    "            lng = geo_data.get('lng', None)\n",
    "            if lat: lat = float(lat)\n",
    "            if lng: lat = float(lng)\n",
    "            return([lat, lng])\n",
    "        else:\n",
    "            log(f\"Cannot read cache file / file does not exist: {cache_file}\", 1)\n",
    "            return(False)\n",
    "\n",
    "def get_native_tweet_location(tweet):\n",
    "    if tweet.geo is not None and tweet.geo.get('coordinates', None):\n",
    "        coordinates = tweet.geo.get('coordinates', None)\n",
    "        if coordinates: coordinates = [float(x) for x in coordinates]\n",
    "        return(coordinates)\n",
    "    else:\n",
    "        return(False)\n",
    "\n",
    "def return_keywords_in_bounding_box(tweets, location = None, boundingbox = [], include_inferred = True, filter = True):\n",
    "    all_keywords, total_processed_tweets = {}, 0\n",
    "    if not location and not boundingbox: raise RuntimeError(\"Location or boundingbox must be provided.\")\n",
    "    if location and boundingbox: raise RuntimeError(\"Location or boundingbox must be provided, not both.\")\n",
    "\n",
    "    # We have been provided with location, let's extract boundingbox\n",
    "    if location: boundingbox = get_boundingbox(location = location)\n",
    "        \n",
    "    # We have been provided with boundingbox, let's make sure datatypes are correct\n",
    "    if boundingbox and isinstance(boundingbox, list): boundingbox = [float(x) for x in boundingbox]\n",
    "    \n",
    "    if filter:\n",
    "        total_unfiltered_tweets = len(tweets)\n",
    "        log(f\"Filtering {total_unfiltered_tweets} tweets.\", 1)\n",
    "        tweets = [x for x in tweets \n",
    "                      if not TAGS.clean_text(x.full_text).startswith(\"rt\") \n",
    "                      and \"boylesque\" in TAGS.clean_text(x.full_text)\n",
    "                 ]\n",
    "        total_num_tweets = len(tweets)\n",
    "        log(f\"=> {total_num_tweets} filtered tweets.\", 1)\n",
    "    \n",
    "    bar = progressbar.ProgressBar(maxval=total_num_tweets).start()\n",
    "    for i, tweet in enumerate(tweets):\n",
    "        clear_output()\n",
    "        bar.update(i)\n",
    "        if get_native_tweet_location(tweet):\n",
    "            lat, lng = get_native_tweet_location(tweet)[0], get_native_tweet_location(tweet)[1]\n",
    "        elif include_inferred and get_user_location(tweet):\n",
    "            lat, lng = get_user_location(tweet)[0], get_user_location(tweet)[1]\n",
    "        else:\n",
    "            lat, lng = None, None\n",
    "\n",
    "        # check if lat + lng + boundingbox exists\n",
    "        if lat and lng and boundingbox:\n",
    "            # TODO: test for type not float....!\n",
    "            if (boundingbox[1] > lat > boundingbox[0]) and (boundingbox[3] > lng > boundingbox[2]):\n",
    "                full_text = tweet.full_text.replace(\"amp\", \"\")\n",
    "                keywords = TAGS.clean_text(full_text)\n",
    "\n",
    "                for keyword in keywords.split(\" \"):\n",
    "                    if keyword not in all_keywords: all_keywords[keyword] = 0\n",
    "                    all_keywords[keyword] += 1\n",
    "                total_processed_tweets += 1\n",
    "    bar.finish()\n",
    "\n",
    "    return({'total_unfiltered_tweets': total_unfiltered_tweets, 'total_num_tweets': total_num_tweets, 'total_processed_tweets': total_processed_tweets, 'all_keywords': all_keywords})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-656-a31ce161dcab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# return_keywords_in_bounding_box(tweets.tweets, boundingbox=['24.76' ,'49.21', '-128.232422', '-66.88']) # = retrieves all keywords from the U.S... not very precise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# return_keywords_in_bounding_box(tweets.tweets, boundingbox=['31', '49.21', '-128', '-116']) # = retrieves keywords from the west coast of U.S.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mreturn_keywords_in_bounding_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboundingbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'25'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'49.21'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-81'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-66.5'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# = retrieves keywords from the east coast of U.S.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-655-e782ed13c50d>\u001b[0m in \u001b[0;36mreturn_keywords_in_bounding_box\u001b[0;34m(tweets, location, boundingbox, include_inferred, filter)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# check if lat + lng + boundingbox exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlng\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mboundingbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mboundingbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlat\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mboundingbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mboundingbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlng\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mboundingbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0mfull_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"amp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0mkeywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "# return_keywords_in_bounding_box(tweets.tweets, location = \"colchester\") # retrieve all the keywords from colchester..\n",
    "# return_keywords_in_bounding_box(tweets.tweets, location='Austin, TX') # = retrieves all keywords from Austin, TX\n",
    "# you can also use include_inferred parameter to exclude those tweets where location is inferred through the user's fill-in location [although I am not sure that works just yet]\n",
    "\n",
    "# [min_lat, max_lat, min_lng, max_lng]\n",
    "# A couple of boundingboxes:\n",
    "#  - UK: ['51.765706', '59.645540', '-11.601563', '1.026803']\n",
    "#  - US: ['24.76' ,'49.21', '-128.232422', '-66.88']\n",
    "#       east coast: ['25', '49.21', '-81', '-66.5']\n",
    "#       west coast: ['31', '49.21', '-128', '-116']\n",
    "\n",
    "\n",
    "# Example: Get all keywords from tweets from UK: return_keywords_in_bounding_box(tweets.tweets, boundingbox=['51.765706', '59.645540', '-11.601563', '1.026803'])\n",
    "# return_keywords_in_bounding_box(tweets.tweets, boundingbox=['51.765706' ,'59.645540', '-128.232422', '1.026803'])\n",
    "\n",
    "# return_keywords_in_bounding_box(tweets.tweets, boundingbox=['24.76' ,'49.21', '-128.232422', '-66.88']) # = retrieves all keywords from the U.S... not very precise\n",
    "# return_keywords_in_bounding_box(tweets.tweets, boundingbox=['31', '49.21', '-128', '-116']) # = retrieves keywords from the west coast of U.S. \n",
    "return_keywords_in_bounding_box(tweets.tweets, boundingbox=['25', '49.21', '-81', '-66.5']) # = retrieves keywords from the east coast of U.S. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
